# ğŸ¤– RAG Learning Experiments

A clean, modular RAG (Retrieval-Augmented Generation) system for learning and experimentation. Compare TF-IDF vs modern embeddings with real data and interactive demos.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Features

- **ğŸ“Š Side-by-Side Comparison**: TF-IDF vs Embeddings with real metrics
- **ğŸ” Interactive Testing**: Ask questions and see both methods respond
- **ğŸ“ˆ Performance Analytics**: Response times, accuracy, similarity scores
- **ğŸ¨ Beautiful Web Interface**: Streamlit app with charts and visualizations
- **ğŸ“ Ready for Articles**: Perfect for blog posts and Medium articles
- **ğŸ—ï¸ Clean Architecture**: Modular, extensible, well-documented code

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- [Ollama](https://ollama.ai/) installed and running

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/rag-learning-experiments.git
cd rag-learning-experiments
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Setup Ollama (Local LLM)**
```bash
# Install Ollama from https://ollama.ai/
ollama pull llama3.2
ollama serve
```

4. **Initialize sample data**
```bash
python -c "from src.datasets import setup_sample_data; setup_sample_data()"
```

5. **Launch the web app**
```bash
streamlit run streamlit_app.py
```

## ğŸ“Š What You'll Learn

### TF-IDF vs Embeddings Showdown
See the real differences between traditional and modern retrieval:

| Method | Speed | Semantic Understanding | Setup Complexity |
|--------|--------|----------------------|------------------|
| **TF-IDF** | âš¡ Very Fast | âŒ Keyword only | âœ… Simple |
| **Embeddings** | ğŸŒ Slower | âœ… Understands meaning | ğŸ”§ Moderate |

### Example Results
```
Query: "Should I use RAG or fine-tune my model?"

TF-IDF: 
  â”œâ”€â”€ Similarity: 0.234
  â”œâ”€â”€ Time: 0.12s  
  â””â”€â”€ Found: 2 docs

Embeddings:
  â”œâ”€â”€ Similarity: 0.789
  â”œâ”€â”€ Time: 0.18s
  â””â”€â”€ Found: 3 docs
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataset   â”‚â”€â”€â”€â–¶â”‚   Retriever  â”‚â”€â”€â”€â–¶â”‚     LLM     â”‚
â”‚  (JSON)     â”‚    â”‚(TF-IDF/Embed)â”‚    â”‚  (Ollama)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG System                              â”‚
â”‚           (Orchestrates Everything)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Evaluation    â”‚    â”‚   Streamlit     â”‚
â”‚   & Metrics     â”‚    â”‚   Web App       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“± Usage Examples

### Command Line Experiments
```python
# Quick comparison
from src.evaluation import compare_single_question
results = compare_single_question("What is RAG?")

# Full experiment
from src.evaluation import run_quick_experiment  
experiment_results = run_quick_experiment()

# Custom dataset
from src.rag_core import create_embedding_rag
from src.datasets import DatasetLoader

loader = DatasetLoader()
documents = loader.load_qa_dataset("your_data.json")
rag = create_embedding_rag(documents)
result = rag.ask("Your question here")
```

### Interactive Web App
1. ğŸ” **Single Question Mode**: Test individual queries
2. ğŸ“Š **Batch Comparison**: Test multiple questions at once  
3. ğŸ§ª **Full Experiment**: Comprehensive analysis with saved results

## ğŸ“ Project Structure

```
rag-learning-experiments/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_core.py          # Complete RAG system
â”‚   â”œâ”€â”€ datasets.py          # Data loading utilities
â”‚   â””â”€â”€ evaluation.py        # Experiments & metrics
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ai_engineering_qa.json    # AI Q&A dataset (10 items)
â”‚   â””â”€â”€ test_questions.json       # Evaluation questions (15 items)
â”œâ”€â”€ streamlit_app.py         # Interactive web interface
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ results/                 # Experiment results (auto-created)
```

## ğŸ§ª Experiments You Can Run

### 1. Method Comparison
Compare TF-IDF vs embeddings on identical data:
```bash
python -c "from src.evaluation import run_quick_experiment; run_quick_experiment()"
```

### 2. Custom Questions
Test your own questions:
```python
from src.evaluation import compare_single_question
compare_single_question("Your custom question here")
```

### 3. Parameter Tuning
Experiment with different settings:
```python
from src.rag_core import TFIDFRetriever, EmbeddingRetriever

# Try different TF-IDF settings
retriever = TFIDFRetriever(max_features=500, similarity_threshold=0.2)

# Try different embedding models  
retriever = EmbeddingRetriever("all-mpnet-base-v2")
```

## ğŸ“Š Sample Results

Based on AI engineering Q&A dataset:

| Metric | TF-IDF | Embeddings | Winner |
|--------|--------|------------|--------|
| Success Rate | 73% | 87% | ğŸ¤— Embeddings |
| Avg Response Time | 0.12s | 0.18s | âš¡ TF-IDF |
| Avg Similarity | 0.234 | 0.789 | ğŸ¤— Embeddings |

**Key Insights:**
- Embeddings understand semantic meaning better
- TF-IDF is faster but misses context
- For learning/prototyping: Start with TF-IDF, upgrade to embeddings
- For production: Embeddings usually worth the extra compute cost

## ğŸ› ï¸ Customization

### Add Your Own Dataset
1. Create JSON file in `data/` directory:
```json
[
  {
    "id": "unique_id", 
    "question": "Your question?",
    "answer": "Your answer...",
    "category": "optional_category",
    "tags": ["tag1", "tag2"]
  }
]
```

2. Load and use:
```python
from src.datasets import DatasetLoader
loader = DatasetLoader()
docs = loader.load_qa_dataset("your_dataset.json")
```

### Try Different Models
```python
# Different embedding models
EmbeddingRetriever("all-mpnet-base-v2")      # Better quality
EmbeddingRetriever("multi-qa-MiniLM-L6-cos-v1")  # Q&A optimized

# Different LLM models (requires Ollama)
LLMInterface("mistral")     # Alternative model
LLMInterface("codellama")   # Code-focused model
```

## ğŸ“ Learning Path

1. **Week 1**: Run basic comparisons, understand the differences
2. **Week 2**: Add your own Q&A data, see how methods perform
3. **Week 3**: Experiment with different embedding models
4. **Week 4**: Try parameter tuning and optimization
5. **Week 5**: Build something new with the components!

## ğŸ“ Blog Post Ideas

This repository is perfect for writing about:
- "TF-IDF vs Embeddings: A Head-to-Head Comparison"
- "Building Your First RAG System from Scratch"  
- "Why Your RAG System Needs Better Retrieval"
- "Local vs Cloud: The RAG Cost Analysis"

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Add your improvements (new datasets, methods, visualizations)
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push to branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

**Ideas for contributions:**
- New datasets (medical, legal, technical domains)
- Additional retrieval methods (BM25, hybrid approaches)
- More LLM integrations (OpenAI, Anthropic, local models)
- Advanced evaluation metrics
- Performance optimizations

## ğŸ› Troubleshooting

### Common Issues

**"Cannot import RAG modules"**
- Make sure you're running from the project root directory
- Check that all dependencies are installed: `pip install -r requirements.txt`

**"Dataset file not found"**
- Run the data setup: `python -c "from src.datasets import setup_sample_data; setup_sample_data()"`

**"Ollama connection error"**
- Make sure Ollama is installed and running: `ollama serve`
- Verify the model is downloaded: `ollama pull llama3.2`

**Slow embedding model loading**
- First run downloads the model (~90MB for all-MiniLM-L6-v2)
- Subsequent runs use cached model and are much faster

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [LangChain](https://github.com/langchain-ai/langchain) for the RAG framework
- [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) for embeddings
- [Ollama](https://ollama.ai/) for local LLM serving
- [Streamlit](https://streamlit.io/) for the beautiful web interface

## ğŸ“§ Contact

Have questions or want to collaborate? 
- Create an issue in this repository
- Share your experiments and results!
- Tag me in your blog posts about RAG learning

---

â­ **Star this repo if it helped you learn about RAG systems!** â­