# RAG Learning Lab

Interactive Streamlit app for comparing RAG system approaches.

## Quick Setup

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Install and start Ollama:**
   ```bash
   # Install Ollama (see https://ollama.ai)
   ollama pull llama3.2
   ```

3. **Run the app:**
   ```bash
   streamlit run streamlit_app.py
   ```

## Features

- ðŸ”§ **TF-IDF vs Embeddings**: Compare different retrieval methods
- ðŸ“Š **Generation Quality**: Evaluate answer accuracy against expected responses  
- ðŸŽ¯ **Interactive UI**: Clean interface with real-time comparisons
- ðŸ“ˆ **Batch Testing**: Test multiple questions at once

## Sample Questions

Try these questions to see generation quality metrics:

- "Should I use RAG or fine-tune my language model?"
- "How much do OpenAI embeddings cost compared to local models?"
- "Why is my RAG system making things up?"

## Project Structure

```
â”œâ”€â”€ streamlit_app.py          # Main Streamlit application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_core.py          # RAG system implementation
â”‚   â”œâ”€â”€ datasets.py          # Dataset loading utilities
â”‚   â””â”€â”€ evaluation.py       # Evaluation and metrics
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ai_engineering_qa.json    # Q&A dataset
â”‚   â””â”€â”€ test_questions.json       # Test questions
â””â”€â”€ results/                 # Experiment results
```

## Troubleshooting

- **Import errors**: Make sure you're running from the project root
- **Ollama errors**: Ensure Ollama is running and llama3.2 is installed
- **No generation quality**: Questions must match dataset entries exactly

Happy experimenting! ðŸš€
